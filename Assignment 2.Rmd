---
title: "Data Science for Industry - Assignment 2"
output: html_notebook
---

#Text Mining on Sona Speeches 

##1. Introduction

The State of the Nation Address (SONA) is an annual event in South Africa which drives significant political and economic impact, since the president reports on the current status of the country. Gaining deeper insight into the various speeches from different presidents can provide a foundation for political prediction engines. A convolutional neural network is used to predict which president was the source of a statement, given a particular sentence as an input. This can provide insight into the similarities and trends between different presidents' speeches, and determining possible overlap between key statements issues annually. [XXXSimon StuffXXX]

##2. Convolutional Neural Network

Neural networks have gained popularity in recent times for complex classifications, due to advancements in distributed computing environments. Furthermore, neural nets allow one to artificially model the way the human brain makes decisions, which address complex classification tasks such as vision, speech, and natural language processing. 

The President Predictor is built using the `R Keras` neural network library.

###2.1 Data Preprocessing

Firstly, the data is read in and converted into an appropriate dataframe form. This involves storing the entire speech as a row entry of a dataframe.

```{r}
rm(list = ls())
library(dplyr)
library(stringr)
txt_files <- list.files("/Users/prinpillay/Desktop/DSFI - Assignment 2/Data/sona-text-1994-2018/")
sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("/Users/prinpillay/Desktop/DSFI - Assignment 2/Data/sona-text-1994-2018/", i)
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  # make a single dataset
  sona <- rbind(sona, this_sona)
}
```

We then perform extra processing to determine the Year and the name of the President for a particular speech.

```{r}
#Extract the year
sona$year <- str_sub(sona$filename, start = 1, end = 4)
#Extract all names removing txt
sona$president <- str_sub(sona$filename, start = 6, end=-5)
#Now we remove pre and post election prefixes
sona$president[str_detect(sona$pre,'post')]=str_sub(sona$president[str_detect(sona$pre, 'post')], start = 16)
sona$president[str_detect(sona$pre,'pre')]=str_sub(sona$president[str_detect(sona$pre, 'pre')], start = 15)
```

We then use tokenization to unnest the speech into sentences which can be used as inputs to the neural network model.

```{r}
library(tidytext)
tidy_sona <- sona %>% unnest_tokens(text, speech, token = "sentences")
```

Exploratory analysis is used to determine the appropriate hyperparameters that will be used in the CNN model. This involves determiing the unique number of words in all the speeches, the longest sentence and the frequency of particular words. The process of string manipulation involves unnesting the speech row into words, and then extracting the required features.

```{r}

# Apply a sentence ID to the dataframe
tidy_sona$ID <- seq.int(nrow(tidy_sona))
#Unnest the data into words
tidy_sona2 = tidy_sona %>% unnest_tokens(text, text, token = "words")
#Check the most frequent words in all the text, according to sentence ID
tidy_sona2 %>% group_by(ID, text)  %>% summarize(count = n()) %>% arrange(desc(count))
#Check for the longest sentence
tidy_sona2 %>% group_by(ID)  %>% summarize(count = n()) %>% arrange(desc(count))
#Check for all unique words (count)
length(unique(tidy_sona2$text))

```

###2.2 Model Construction

Once baseline parameters surrounding the data are established, we proceed with initializing the CNN with hyperparameters required for the model. 


```{r}

#choose max_features most popular words
max_features <- 10625
# exclude sentences shorter than this
minlen <- 5 
# longest sentence (for padding)
maxlen <- 340             
# Number of unique words
input_dimensions <- 10625   
#Random output dimension space
embedding_dims = 100       
# Longest sentence
input_length = 340       

#Convert text to digits
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, tidy_sona$text)
sequences = tokenizer$texts_to_sequences(tidy_sona$text)

#Remove short sentences
seq_ok <- unlist(lapply(sequences, length)) > minlen
lengthIs <- function(n) function(x) length(x)>n
sequences <- Filter(lengthIs(minlen), sequences)

#Convert outputs to multiclass integer
y <- as.numeric(as.factor(tidy_sona$president[seq_ok]))
y = as.integer(y)


#Generate test and train set
train <- list()
test<- list()

train_id <- sample(1:length(sequences),
                size = 0.9*length(sequences), 
                replace=F)

test$x <-  sequences[-train_id]
train$x <- sequences[train_id]
train$y <- y[train_id]
test$y <-  y[-train_id]
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)

y_train= to_categorical(train$y)
y_test <- to_categorical(test$y)

```

Build model

```{r}
model <- keras_model_sequential()

model %>%
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  # add some dropout
  layer_dropout(0.2) %>%
  # convolutional layer
  layer_conv_1d(
    filters = 250,
    kernel_size = 3,
    padding = "valid",  # "valid" means no padding, as we did it already
    activation = "relu",
    strides = 1
  ) %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(128) %>%
  layer_dropout(0.2) %>%
  layer_activation("relu") %>%
  layer_dense(7) %>%   # 7 possibilities for output layer
  layer_activation("softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

model %>%
  fit(
    x_train, y_train,
    batch_size = 32,
    epochs = 10,
  )


model %>% evaluate(x_test, y_test)
```

