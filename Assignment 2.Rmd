---
title: "Data Science for Industry - Assignment 2"
output: html_notebook
---

```{r}
#Initialize environment and load libraries
rm(list = ls())
library(dplyr)
library(stringr)
library(tidyverse)
library(keras)
```


#Text Mining on Sona Speeches 

##1. Introduction

The State of the Nation Address (SONA) is an annual event in South Africa which drives significant political and economic impact, since the president reports on the current status of the country. Gaining deeper insight into the various speeches from different presidents can provide a foundation for political prediction engines. A convolutional neural network is used to predict which president was the source of a statement, given a particular sentence as an input. This can provide insight into the similarities and trends between different presidents' speeches, and determining possible overlap between key statements issues annually. [XXXSimon StuffXXX]

##2. Convolutional Neural Network

Neural networks have gained popularity in recent times for complex classifications, due to advancements in distributed computing environments. Furthermore, neural nets allow one to artificially model the way the human brain makes decisions, which address complex classification tasks such as vision, speech, and natural language processing. 

The President Predictor is built using the `R Keras` neural network library.

###2.1 Data Preprocessing

Firstly, the data is read in and converted into an appropriate dataframe form. This involves storing the entire speech as a row entry of a dataframe.

```{r}
txt_files <- list.files("/Users/prinpillay/Desktop/DSFI - Assignment 2/Data/sona-text-1994-2018/")
sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("/Users/prinpillay/Desktop/DSFI - Assignment 2/Data/sona-text-1994-2018/", i)
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  # make a single dataset
  sona <- rbind(sona, this_sona)
}
```

We then perform extra processing to determine the Year and the name of the President for a particular speech.

```{r}
#Extract the year
sona$year <- str_sub(sona$filename, start = 1, end = 4)
#Extract all names removing txt
sona$president <- str_sub(sona$filename, start = 6, end=-5)
#Now we remove pre and post election prefixes
sona$president[str_detect(sona$pre,'post')]=str_sub(sona$president[str_detect(sona$pre, 'post')], start = 16)
sona$president[str_detect(sona$pre,'pre')]=str_sub(sona$president[str_detect(sona$pre, 'pre')], start = 15)
```

We then use tokenization to unnest the speech into sentences which can be used as inputs to the neural network model.

```{r}
library(tidytext)
tidy_sona <- sona %>% unnest_tokens(text, speech, token = "sentences")
```

Exploratory analysis is used to determine the appropriate hyperparameters that will be used in the CNN model. This involves determiing the unique number of words in all the speeches, the longest sentence and the frequency of particular words. The process of string manipulation involves unnesting the speech row into words, and then extracting the required features.

```{r}

# Apply a sentence ID to the dataframe
tidy_sona$ID <- seq.int(nrow(tidy_sona))
#Unnest the data into words
tidy_sona2 = tidy_sona %>% unnest_tokens(text, text, token = "words")
#Check the most frequent words in all the text, according to sentence ID
tidy_sona2 %>% group_by(ID, text)  %>% summarize(count = n()) %>% arrange(desc(count))
#Check for the longest sentence
tidy_sona2 %>% group_by(ID)  %>% summarize(count = n()) %>% arrange(desc(count))
#Check for all unique words (count)
length(unique(tidy_sona2$text))
```

###2.2 Model Construction

Once baseline parameters surrounding the data are established, we proceed with initializing the CNN with hyperparameters required for the model. CNNs are particularly useful in the case of text mining, since they exploit not only the prescence of certain words as predictors, but also the relationship between words. A part of this initialisation phase includes setting the number of features (popular words), longest sentence, and any exclusions for particularly short sentences. Another important feature of neural nets is setting the embedding dimensions. This maps the text input to a higher dimensional feature space, which almost acts as a lookup table for the classifier.

```{r}
#choose max_features most popular words
max_features <- 10625
# exclude sentences shorter than this
minlen <- 5 
# longest sentence (for padding)
maxlen <- 340             
# Number of unique words
input_dimensions <- 10625   
#Random output dimension space
embedding_dims = 100       
# Longest sentence
input_length = 340       
```

We now need to transform the inputs and outputs into a form which can be fed into the CNN. This involves mapping the sentences into digits, with each digit representing a unique word across all sentences in speeches. This assists the CNN in numerically mapping the words as tensors which can be used to perform numerical estimations. In terms of the outputs, the names of the various presidents need to be converted to categorical information.
```{r}
#Convert text to digits based on the maximum number of features
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, tidy_sona$text)
sequences = tokenizer$texts_to_sequences(tidy_sona$text)

#Remove short sentences
seq_ok <- unlist(lapply(sequences, length)) > minlen
lengthIs <- function(n) function(x) length(x)>n
sequences <- Filter(lengthIs(minlen), sequences)

#Convert outputs to multiclass integer
y <- as.numeric(as.factor(tidy_sona$president[seq_ok]))
y = as.integer(y)
```

The next important step in model construction is splitting the data into a training and validation set. This is valuable to determine the out-of-sample error obtained when performing hyperparameter tuning for optimal model conditions. For this particular sample, a 90/10 split was used where 90% of the data is used for training, and 10% of the data is used for validation. Considering that text mining is a relatively difficult classification problem, we want to make as much data available for the training process to enhance the prediction accuracy.

```{r}
#Generate test and train set
train <- list()
test<- list()
#Perform a 90/10 split for training/validation
train_id <- sample(1:length(sequences),
                size = 0.9*length(sequences), 
                replace=F)
test$x <-  sequences[-train_id]
train$x <- sequences[train_id]

#We now also perform a split on the output 
train$y <- y[train_id]
test$y <-  y[-train_id]
```

We also pad the inputs to the length of the longest sentences, to ensure all inputs are the same size.

```{r}
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)

```
Another important step is transforming the numerical outputs into a binary classification matrix as required by the Keras CNN.
```{r}
#Transform train and test outputs into binary classification matrix
y_train= to_categorical(train$y)
y_test <- to_categorical(test$y)
```

We now use `Keras` to build the actual model. In terms of the model architecture, a 1D convolutional Neural Net is used with dropout regularization to prevent overfitting. A grid based hyper-parameter tuning technique is used, where a range of values were tested for optimal accuracy.

```{r}
model <- keras_model_sequential()

model %>%
  # embedding layer maps all the features to the higher dimensional space for lookup
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  # add some dropout
  layer_dropout(0.2) %>%
  # convolutional layer
  layer_conv_1d(
    filters = 250,
    kernel_size = 3,
    padding = "valid",  # "valid" means no padding, as we did it already
    activation = "relu",
    strides = 1
  ) %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(128) %>%
  layer_dropout(0.2) %>%
  layer_activation("relu") %>%
  layer_dense(7) %>%   # 7 possibilities for output layer
  layer_activation("softmax")

?layer_conv_1d

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

model %>%
  fit(
    x_train, y_train,
    batch_size = 32,
    epochs = 10,
  )


model %>% evaluate(x_test, y_test)
```

